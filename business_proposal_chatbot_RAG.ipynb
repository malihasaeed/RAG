{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1NNuCnLiEJHT1cGeH10K0PAst3iyusP0B",
      "authorship_tag": "ABX9TyMxhVS0gcYRerMukyeoILBO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malihasaeed/RAG/blob/main/Copy_of_RAG_assignment1_atomcamp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsN0OPLyJlIw"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain\n",
        "# Requires Python 3.10+"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U groq langchain-groq\n"
      ],
      "metadata": {
        "id": "4x4YdPGDB2za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \\\n",
        "    langchain \\\n",
        "    langchain-community \\\n",
        "    langchain-core \\\n",
        "    langchain-groq \\\n",
        "    faiss-cpu \\\n",
        "    pypdf \\\n",
        "    gradio"
      ],
      "metadata": {
        "id": "lJT6UhEPGs_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = \"rag_chatbot\"\n"
      ],
      "metadata": {
        "id": "zMeggCh9DPxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This code saves your secret API key, then creates a smart-robot helper (llm) so you can ask it questions.\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# read your secret by its name\n",
        "groq_key = userdata.get(\"rag_chatbot\")\n",
        "\n",
        "# set the official env var Groq expects\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_key\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "\n",
        "# llm.invoke(...) is a function (a built-in action) that sends your question to the AI and brings back the answer, which is then printed\n",
        "response = llm.invoke(\"Explain retrieval-augmented generation in simple words.\")\n",
        "#.content pulls out ONLY the words the AI wrote so you can print them.\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "V71B8ohUKGCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "C4qk5k3xkZZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is standard Python code commonly used in tutorials / documentation to read PDFs using PyPDF2.\n",
        "#Itâ€™s not specific to any AI tool.\n",
        "#converting the doc into text\n",
        "!pip install PyPDF2\n",
        "\n",
        "import PyPDF2\n",
        "\n",
        "pdf_file = open(\"business proposal.pdf\", \"rb\")\n",
        "reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "text = \"\"\n",
        "\n",
        "for page in reader.pages:\n",
        "    text += page.extract_text()\n",
        "\n",
        "pdf_file.close()\n",
        "\n",
        "print(text[:500])  # show first 500 chars\n"
      ],
      "metadata": {
        "id": "1kLyRUkFLtqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "print(langchain.__version__)\n"
      ],
      "metadata": {
        "id": "Aybm9rCeATkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "print(langchain.__version__)\n"
      ],
      "metadata": {
        "id": "l5r1eBKGAn98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-text-splitters langchain-core\n"
      ],
      "metadata": {
        "id": "0J6IOJ2FDiec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "what logic is being used?\n",
        "\n",
        "ðŸ”¹ PDFs can be VERY long\n",
        "\n",
        "ðŸ”¹ LLMs cannot read huge text at once\n",
        "\n",
        "ðŸ”¹ So we break it into small readable pieces (chunks)\n",
        "\n",
        "This makes RAG / chatbots work better and faster ðŸš€"
      ],
      "metadata": {
        "id": "Ighxn4DQIoCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is a tool from LangChain that breaks long text into smaller pieces (chunks).\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "#This wraps your text into a Document object â€” a neat container LangChain likes to work with.text=documemt box\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Wrap your extracted text into a Document\n",
        "#Creates a document that stores your PDF text\n",
        "docs = [Document(page_content=text)]\n",
        "\n",
        "# Create the splitter\n",
        "#each text piece can have up to 800 characters\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=200,\n",
        ")\n",
        "\n",
        "# Split into chunks\n",
        "split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "print(\"Total chunks:\", len(split_docs))\n",
        "print(\"First chunk:\\n\", split_docs[0].page_content[:500])\n"
      ],
      "metadata": {
        "id": "ZLFzHxaODrdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-community faiss-cpu sentence-transformers\n"
      ],
      "metadata": {
        "id": "ehCTLl_3D_RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are creating an object called embeddings.\n",
        "\n",
        "Inside, you tell it which model to use:\n",
        "\"sentence-transformers/all-MiniLM-L6-v2\".\n",
        "\n",
        "What this object does:\n",
        "\n",
        "You give it a sentence or chunk of text â†’\n",
        "\n",
        "It returns a vector (a long list of numbers) that represents the meaning of that text.\n",
        "\n",
        "example:\n",
        "Imagine each sentence is a student, and embeddings give each student a score-card of numbers describing who they are. Similar students â†’ similar score-cards."
      ],
      "metadata": {
        "id": "H6ulR5JbOUFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import embeddings + FAISS and build the index\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Create an embeddings model (lightweight, good for Colab)\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# Build the FAISS vector store from your chunks\n",
        "#Stores all those vectors inside a FAISS database â†’ vectorstore\n",
        "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
        "\n",
        "# Turn it into a retriever (k = how many chunks to fetch per question)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "print(\"Vector store built! âœ…\")\n"
      ],
      "metadata": {
        "id": "tH4OC9lrEEfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain\n",
        "!pip install -qU \"langchain>=0.3\" \"langchain-community>=0.3\" langchain-core\n"
      ],
      "metadata": {
        "id": "5O4VHZPqFbke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \"langchain==0.2.14\" \"langchain-community==0.2.12\"\n"
      ],
      "metadata": {
        "id": "1VwXNGn5HPg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple in-memory chat history(object)\n",
        "#It creates a simple list named chat_history to store past messages as (role, text) pairs so the program can remember the conversation.\n",
        "chat_history = []  # list of (role, text) tuples, e.g. (\"user\", \"...\"), (\"assistant\", \"...\")\n"
      ],
      "metadata": {
        "id": "rjvZdZsrIFXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper to format chat history for the prompt\n",
        "def format_chat_history(history):\n",
        "    if not history:\n",
        "        return \"No previous conversation.\"\n",
        "    lines = []\n",
        "    for role, text in history:\n",
        "        lines.append(f\"{role.capitalize()}: {text}\")\n",
        "    return \"\\n\".join(lines)\n"
      ],
      "metadata": {
        "id": "8Vz48039IFFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define system instructions for the assistant\n",
        "\n",
        "SYSTEM_INSTRUCTIONS = \"\"\"\n",
        "You are a helpful Product Support Assistant for a single business proposal document.\n",
        "You must answer ONLY using the information from the provided context.\n",
        "\n",
        "If the answer is NOT clearly supported by the context, say exactly:\n",
        "\"I couldnâ€™t find this information in the available documents.\"\n",
        "\n",
        "Be concise and clear.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "O9S9RE75INYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_with_rag(question: str) -> str:\n",
        "    global chat_history\n",
        "\n",
        "    # 1) Retrieve relevant chunks directly from the FAISS vector store\n",
        "    #    instead of using retriever.get_relevant_documents(...)\n",
        "    docs = vectorstore.similarity_search(question, k=4)\n",
        "\n",
        "    if not docs:\n",
        "        answer = \"I couldnâ€™t find this information in the available documents.\"\n",
        "        chat_history.append((\"user\", question))\n",
        "        chat_history.append((\"assistant\", answer))\n",
        "        return answer\n",
        "\n",
        "    context_text = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "    # 2) Prepare chat history text for the prompt\n",
        "    history_text = format_chat_history(chat_history)\n",
        "\n",
        "    # 3) Build the full prompt\n",
        "    prompt = f\"\"\"{SYSTEM_INSTRUCTIONS}\n",
        "\n",
        "Context from the document:\n",
        "{context_text}\n",
        "\n",
        "Chat history:\n",
        "{history_text}\n",
        "\n",
        "User question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # 4) Call your Groq LLM\n",
        "    response = llm.invoke(prompt)   # ChatGroq -> AIMessage\n",
        "    try:\n",
        "        answer = response.content\n",
        "    except AttributeError:\n",
        "        answer = str(response)\n",
        "\n",
        "    # 5) Update simple memory\n",
        "    chat_history.append((\"user\", question))\n",
        "    chat_history.append((\"assistant\", answer))\n",
        "\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "9b-bOBxjKrJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer_with_rag(\"What is the main objective of this business proposal?\"))\n"
      ],
      "metadata": {
        "id": "yr6r7uWnKwFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer_with_rag(\"what do you offer\"))"
      ],
      "metadata": {
        "id": "BdJhvWQRLWLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Global chat history\n",
        "chat_history = []\n",
        "\n",
        "# 2) The helper function\n",
        "def format_chat_history(history):\n",
        "    if not history:\n",
        "        return \"No previous conversation.\"\n",
        "    lines = []\n",
        "    for role, text in history:\n",
        "        lines.append(f\"{role.capitalize()}: {text}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# 3) System instructions\n",
        "SYSTEM_INSTRUCTIONS = \"\"\"\n",
        "You are a helpful Product Support Assistant for a single business proposal document.\n",
        "You must answer ONLY using the information from the provided context.\n",
        "\n",
        "If the answer is NOT clearly supported by the context, say exactly:\n",
        "\"I couldnâ€™t find this information in the available documents.\"\n",
        "\n",
        "Be concise and clear.\n",
        "\"\"\"\n",
        "\n",
        "# 4) Your Groq model\n",
        "# llm = ChatGroq(...)\n",
        "\n",
        "# 5) vectorstore built from your chunks\n",
        "# vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
        "\n",
        "# 6) The latest version of answer_with_rag using vectorstore.similarity_search\n",
        "def answer_with_rag(question: str) -> str:\n",
        "    global chat_history\n",
        "\n",
        "    docs = vectorstore.similarity_search(question, k=4)\n",
        "\n",
        "    if not docs:\n",
        "        answer = \"I couldnâ€™t find this information in the available documents.\"\n",
        "        chat_history.append((\"user\", question))\n",
        "        chat_history.append((\"assistant\", answer))\n",
        "        return answer\n",
        "\n",
        "    context_text = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "    history_text = format_chat_history(chat_history)\n",
        "\n",
        "    prompt = f\"\"\"{SYSTEM_INSTRUCTIONS}\n",
        "\n",
        "Context from the document:\n",
        "{context_text}\n",
        "\n",
        "Chat history:\n",
        "{history_text}\n",
        "\n",
        "User question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    try:\n",
        "        answer = response.content\n",
        "    except AttributeError:\n",
        "        answer = str(response)\n",
        "\n",
        "    chat_history.append((\"user\", question))\n",
        "    chat_history.append((\"assistant\", answer))\n",
        "\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "EEF2YRZ2NDfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer_with_rag(\"what do you offer\"))"
      ],
      "metadata": {
        "id": "Nakr67RXNJhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n"
      ],
      "metadata": {
        "id": "mq0iZL4DLw4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def gradio_qa(message):\n",
        "    \"\"\"\n",
        "    Simple wrapper: takes user text and returns the RAG answer.\n",
        "    Conversation memory is handled inside answer_with_rag via chat_history.\n",
        "    \"\"\"\n",
        "    return answer_with_rag(message)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=gradio_qa,\n",
        "    inputs=gr.Textbox(lines=2, label=\"Ask about the business proposal\"),\n",
        "    outputs=gr.Textbox(label=\"Assistant\"),\n",
        "    title=\"Business Proposal Support Assistant\",\n",
        "    description=\"Ask questions about the business proposal PDF. Answers come only from that document.\",\n",
        ")\n",
        "\n",
        "demo.launch(share=False)\n"
      ],
      "metadata": {
        "id": "yERFTfryNwVJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
